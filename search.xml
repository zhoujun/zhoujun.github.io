<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[用Python实现一个大数据搜索引擎]]></title>
    <url>%2F2018%2F06%2F09%2Fsimple-bigdata-search-with-python%2F</url>
    <content type="text"><![CDATA[本文转自：https://my.oschina.net/taogang/blog/1579204 搜索是大数据领域里常见的需求。Splunk和ELK分别是该领域在非开源和开源领域里的领导者。本文利用很少的Python代码实现了一个基本的数据搜索功能，试图让大家理解大数据搜索的基本原理。 1、布隆过滤器（Bloom Filter）第一步我们先要实现一个布隆过滤器 布隆过滤器是大数据领域的一个常见算法，它的目的是过滤掉那些不是目标的元素。也就是说如果一个要搜索的词并不存在与我的数据中，那么它可以以很快的速度返回目标不存在。 让我们看看以下布隆过滤器的代码： 1234567891011121314151617181920212223242526272829class Bloomfilter(object): &quot;&quot;&quot; A Bloom filter is a probabilistic data-structure that trades space for accuracy when determining if a value is in a set. It can tell you if a value was possibly added, or if it was definitely not added, but it can&apos;t tell you for certain that it was added. &quot;&quot;&quot; def __init__(self, size): &quot;&quot;&quot;Setup the BF with the appropriate size&quot;&quot;&quot; self.values = [False] * size self.size = size def hash_value(self, value): &quot;&quot;&quot;Hash the value provided and scale it to fit the BF size&quot;&quot;&quot; return hash(value) % self.size def add_value(self, value): &quot;&quot;&quot;Add a value to the BF&quot;&quot;&quot; h = self.hash_value(value) self.values[h] = True def might_contain(self, value): &quot;&quot;&quot;Check if the value might be in the BF&quot;&quot;&quot; h = self.hash_value(value) return self.values[h] def print_contents(self): &quot;&quot;&quot;Dump the contents of the BF for debugging purposes&quot;&quot;&quot; print(self.values) 基本的数据结构是个数组（实际上是个位图，用1/0来记录数据是否存在），初始化是没有任何内容，所以全部置False。实际的使用当中，该数组的长度是非常大的，以保证效率。 利用哈希算法来决定数据应该存在哪一位，也就是数组的索引 当一个数据被加入到布隆过滤器的时候，计算它的哈希值然后把相应的位置为True 当检查一个数据是否已经存在或者说被索引过的时候，只要检查对应的哈希值所在的位的True／Fasle 看到这里，大家应该可以看出，如果布隆过滤器返回False，那么数据一定是没有索引过的，然而如果返回True，那也不能说数据一定就已经被索引过。在搜索过程中使用布隆过滤器可以使得很多没有命中的搜索提前返回来提高效率。 我们看看这段 code是如何运行的： 12345678910bf = Bloomfilter(10)bf.add_value(&apos;dog&apos;)bf.add_value(&apos;fish&apos;)bf.add_value(&apos;cat&apos;)bf.print_contents()bf.add_value(&apos;bird&apos;)bf.print_contents()# Note: contents are unchanged after adding bird - it collidesfor term in [&apos;dog&apos;, &apos;fish&apos;, &apos;cat&apos;, &apos;bird&apos;, &apos;duck&apos;, &apos;emu&apos;]: print(&apos;&#123;&#125;: &#123;&#125; &#123;&#125;&apos;.format(term, bf.hash_value(term), bf.might_contain(term))) 12345678[False, False, False, False, True, True, False, False, False, True][False, False, False, False, True, True, False, False, False, True]dog: 5 Truefish: 4 Truecat: 9 Truebird: 9 Trueduck: 5 Trueemu: 8 False 1.1、首先创建了一个容量为10的的布隆过滤器 1.2、然后分别加入 ‘dog’，‘fish’，‘cat’三个对象，这时的布隆过滤器的内容如下： 1.3、然后加入‘bird’对象，布隆过滤器的内容并没有改变，因为‘bird’和‘fish’恰好拥有相同的哈希。 1.4、最后我们检查一堆对象（’dog’, ‘fish’, ‘cat’, ‘bird’, ‘duck’, ‘emu’）是不是已经被索引了。结果发现‘duck’返回True，2而‘emu’返回False。因为‘duck’的哈希恰好和‘dog’是一样的。 2、分词下面一步我们要实现分词。 分词的目的是要把我们的文本数据分割成可搜索的最小单元，也就是词。这里我们主要针对英语，因为中文的分词涉及到自然语言处理，比较复杂，而英文基本只要用标点符号就好了。 下面我们看看分词的代码： 12345678910111213141516171819202122232425def major_segments(s): &quot;&quot;&quot; Perform major segmenting on a string. Split the string by all of the major breaks, and return the set of everything found. The breaks in this implementation are single characters, but in Splunk proper they can be multiple characters. A set is used because ordering doesn&apos;t matter, and duplicates are bad. &quot;&quot;&quot; major_breaks = &apos; &apos; last = -1 results = set() # enumerate() will give us (0, s[0]), (1, s[1]), ... for idx, ch in enumerate(s): if ch in major_breaks: segment = s[last+1:idx] results.add(segment) last = idx # The last character may not be a break so always capture # the last segment (which may end up being &quot;&quot;, but yolo) segment = s[last+1:] results.add(segment) return results 2.1、主要分割主要分割使用空格来分词，实际的分词逻辑中，还会有其它的分隔符。例如Splunk的缺省分割符包括以下这些，用户也可以定义自己的分割符。 ] &lt; &gt; ( ) { } | ! ; , ‘ “ * \n \r \s \t &amp; ? + %21 %26 %2526 %3B %7C %20 %2B %3D – %2520 %5D %5B %3A %0A %2C %28 %29 12345678910111213141516171819202122232425def minor_segments(s): &quot;&quot;&quot; Perform minor segmenting on a string. This is like major segmenting, except it also captures from the start of the input to each break. &quot;&quot;&quot; minor_breaks = &apos;_.&apos; last = -1 results = set() for idx, ch in enumerate(s): if ch in minor_breaks: segment = s[last+1:idx] results.add(segment) segment = s[:idx] results.add(segment) last = idx segment = s[last+1:] results.add(segment) results.add(s) return results 2.2、次要分割次要分割和主要分割的逻辑类似，只是还会把从开始部分到当前分割的结果加入。例如“1.2.3.4”的次要分割会有1，2，3，4，1.2，1.2.3 1234567def segments(event): &quot;&quot;&quot;Simple wrapper around major_segments / minor_segments&quot;&quot;&quot; results = set() for major in major_segments(event): for minor in minor_segments(major): results.add(minor) return results 分词的逻辑就是对文本先进行主要分割，对每一个主要分割在进行次要分割。然后把所有分出来的词返回。 我们看看这段 code是如何运行的： 12for term in segments(&apos;src_ip = 1.2.3.4&apos;): print(term) 结果： 1234567891011src1.21.2.3.4src_ip311.2.3ip2=4 3、搜索好了，有个分词和布隆过滤器这两个利器的支撑后，我们就可以来实现搜索的功能了。 上代码： 1234567891011121314151617181920212223242526272829303132333435class Splunk(object): def __init__(self): self.bf = Bloomfilter(64) self.terms = &#123;&#125; # Dictionary of term to set of events self.events = [] def add_event(self, event): &quot;&quot;&quot;Adds an event to this object&quot;&quot;&quot; # Generate a unique ID for the event, and save it event_id = len(self.events) self.events.append(event) # Add each term to the bloomfilter, and track the event by each term for term in segments(event): self.bf.add_value(term) if term not in self.terms: self.terms[term] = set() self.terms[term].add(event_id) def search(self, term): &quot;&quot;&quot;Search for a single term, and yield all the events that contain it&quot;&quot;&quot; # In Splunk this runs in O(1), and is likely to be in filesystem cache (memory) if not self.bf.might_contain(term): return # In Splunk this probably runs in O(log N) where N is the number of terms in the tsidx if term not in self.terms: return for event_id in sorted(self.terms[term]): yield self.events[event_id] Splunk代表一个拥有搜索功能的索引集合 每一个集合中包含一个布隆过滤器，一个倒排词表（字典），和一个存储所有事件的数组 当一个事件被加入到索引的时候，会做以下的逻辑 为每一个事件生成一个unqie id，这里就是序号 对事件进行分词，把每一个词加入到倒排词表，也就是每一个词对应的事件的id的映射结构，注意，一个词可能对应多个事件，所以倒排表的的值是一个Set。倒排表是绝大部分搜索引擎的核心功能。 当一个词被搜索的时候，会做以下的逻辑 检查布隆过滤器，如果为假，直接返回 检查词表，如果被搜索单词不在词表中，直接返回 在倒排表中找到所有对应的事件id，然后返回事件的内容 我们运行下看看吧： 12345678910111213s = Splunk()s.add_event(&apos;src_ip = 1.2.3.4&apos;)s.add_event(&apos;src_ip = 5.6.7.8&apos;)s.add_event(&apos;dst_ip = 1.2.3.4&apos;)for event in s.search(&apos;1.2.3.4&apos;): print(event)print(&apos;-&apos;)for event in s.search(&apos;src_ip&apos;): print(event)print(&apos;-&apos;)for event in s.search(&apos;ip&apos;): print(event) 123456789src_ip = 1.2.3.4dst_ip = 1.2.3.4-src_ip = 1.2.3.4src_ip = 5.6.7.8-src_ip = 1.2.3.4src_ip = 5.6.7.8dst_ip = 1.2.3.4 是不是很赞！ 4、更复杂的搜索更进一步，在搜索过程中，我们想用And和Or来实现更复杂的搜索逻辑。 上代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class SplunkM(object): def __init__(self): self.bf = Bloomfilter(64) self.terms = &#123;&#125; # Dictionary of term to set of events self.events = [] def add_event(self, event): &quot;&quot;&quot;Adds an event to this object&quot;&quot;&quot; # Generate a unique ID for the event, and save it event_id = len(self.events) self.events.append(event) # Add each term to the bloomfilter, and track the event by each term for term in segments(event): self.bf.add_value(term) if term not in self.terms: self.terms[term] = set() self.terms[term].add(event_id) def search_all(self, terms): &quot;&quot;&quot;Search for an AND of all terms&quot;&quot;&quot; # Start with the universe of all events... results = set(range(len(self.events))) for term in terms: # If a term isn&apos;t present at all then we can stop looking if not self.bf.might_contain(term): return if term not in self.terms: return # Drop events that don&apos;t match from our results results = results.intersection(self.terms[term]) for event_id in sorted(results): yield self.events[event_id] def search_any(self, terms): &quot;&quot;&quot;Search for an OR of all terms&quot;&quot;&quot; results = set() for term in terms: # If a term isn&apos;t present, we skip it, but don&apos;t stop if not self.bf.might_contain(term): continue if term not in self.terms: continue # Add these events to our results results = results.union(self.terms[term]) for event_id in sorted(results): yield self.events[event_id] 利用Python集合的intersection和union操作，可以很方便的支持And（求交集）和Or（求合集）的操作。 运行结果如下： 12345678910s = SplunkM()s.add_event(&apos;src_ip = 1.2.3.4&apos;)s.add_event(&apos;src_ip = 5.6.7.8&apos;)s.add_event(&apos;dst_ip = 1.2.3.4&apos;)for event in s.search_all([&apos;src_ip&apos;, &apos;5.6&apos;]): print(event)print(&apos;-&apos;)for event in s.search_any([&apos;src_ip&apos;, &apos;dst_ip&apos;]): print(event) 12345src_ip = 5.6.7.8-src_ip = 1.2.3.4src_ip = 5.6.7.8dst_ip = 1.2.3.4 完整源码地址 5、总结以上的代码只是为了说明大数据搜索的基本原理，包括布隆过滤器，分词和倒排表。如果大家真的想要利用这代码来实现真正的搜索功能，还差的太远。所有的内容来自于Splunk Conf2017。大家如果有兴趣可以去看网上的视频。 文档 视频]]></content>
      <categories>
        <category>SearchEngine</category>
      </categories>
      <tags>
        <tag>SearchEngine</tag>
        <tag>Python</tag>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的10个隐藏特性]]></title>
    <url>%2F2018%2F05%2F24%2Fpython-hidden-feature%2F</url>
    <content type="text"><![CDATA[1、函数参数解包(unpack)123456789&gt;&gt;&gt; def foo(x, y): print(x, y)... &gt;&gt;&gt; alist = [1,2]&gt;&gt;&gt; adict = &#123;&apos;x&apos;: 1, &apos;y&apos;: 2&#125;&gt;&gt;&gt; foo(*alist)1 2&gt;&gt;&gt; foo(**adict)1 2&gt;&gt;&gt; 2、python3中的元组unpack12345678&gt;&gt;&gt; a, b, *rest = range(10)&gt;&gt;&gt; a0&gt;&gt;&gt; b1&gt;&gt;&gt; rest[2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; 也可以取最后一个： 123456789&gt;&gt;&gt; first, second, *rest, last = range(10)&gt;&gt;&gt; first0&gt;&gt;&gt; second1&gt;&gt;&gt; last9&gt;&gt;&gt; rest[2, 3, 4, 5, 6, 7, 8] 3、链式比较操作符12345&gt;&gt;&gt; 1 &lt; x &lt; 2False&gt;&gt;&gt; 4 &gt; x &gt;= 3True&gt;&gt;&gt; 4、列表切片操作1234567891011121314151617181920# 按步长2取列表数据&gt;&gt;&gt; a = [1,2,3,4,5]&gt;&gt;&gt; a[::2] [1, 3, 5]# 用步长-1来反转列表&gt;&gt;&gt; a[::-1][5, 4, 3, 2, 1]# 用切片来删除列表的某一段&gt;&gt;&gt; a = [1,2,3,4,5]&gt;&gt;&gt; a[1:3] = []&gt;&gt;&gt; a[1, 4, 5]# 也可以用 del a[1:3]&gt;&gt;&gt; a = [1,2,3,4,5]&gt;&gt;&gt; del a[1:3]&gt;&gt;&gt; a[1, 4, 5] 5、嵌套的列表推导式12&gt;&gt;&gt; [(i, j) for i in range(3) for j in range(i)][(1, 0), (2, 0), (2, 1)] 6、字典里的无限递归12345&gt;&gt;&gt; a, b = &#123;&#125;, &#123;&#125;&gt;&gt;&gt; a[&apos;b&apos;] = b&gt;&gt;&gt; b[&apos;a&apos;] = a&gt;&gt;&gt; a&#123;&apos;b&apos;: &#123;&apos;a&apos;: &#123;...&#125;&#125;&#125; 当然列表也可以 12345&gt;&gt;&gt; a, b = [], []&gt;&gt;&gt; a.append(b)&gt;&gt;&gt; b.append(a)&gt;&gt;&gt; a[[[...]]] 7、下划线”_”123456# _ 在Python解析器上返回上一次的值&gt;&gt;&gt; 1 + 12&gt;&gt;&gt; _2 另外 Python中的”_”也经常用在未使用的变量命名 12&gt;&gt;&gt; pos = (1, 2)&gt;&gt;&gt; x, _ = pos 8、注意函数的默认参数12345678&gt;&gt;&gt; def foo(x=[]):... x.append(1)... print x...&gt;&gt;&gt; foo()[1]&gt;&gt;&gt; foo()[1, 1] 更安全的做法： 1234567891011&gt;&gt;&gt; def foo(x=None):... if x is None:... x = []... x.append(1)... print x...&gt;&gt;&gt; foo()[1]&gt;&gt;&gt; foo()[1]&gt;&gt;&gt; 9、另一种字符串连接123&gt;&gt;&gt; name = &quot;Hello&quot; &quot;World&quot;&gt;&gt;&gt; name&apos;HelloWorld&apos; 连接多行： 1234&gt;&gt;&gt; name = &quot;Hello&quot; \... &quot;World&quot;&gt;&gt;&gt; name&apos;HelloWorld&apos; 10、不能访问到的属性123456789&gt;&gt;&gt; class Foo(object): pass...&gt;&gt;&gt; obj = Foo()&gt;&gt;&gt; setattr(o, &apos;hello world&apos;, 1)&gt;&gt;&gt; o.hello world File &quot;&lt;stdin&gt;&quot;, line 1 o.hello world ^SyntaxError: invalid syntax 不过，能用 setattr 设置属性，就可以用 getattr 取出 12&gt;&gt;&gt; getattr(o, &apos;hello world&apos;)1]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3.6中那些很酷的特性]]></title>
    <url>%2F2018%2F05%2F22%2Fpython3.6-cool-feature%2F</url>
    <content type="text"><![CDATA[可读性更强的数字字面值Python代码在可读性上做到了极致，被称为是可执行伪代码。然而，它还在不断地改进，比如这个可读性更好的数字字面值语法，就是方便程序员能以一种 “for humans ” 的方式阅读和理解数字。你现在可以给数字添加下划线，并按照你喜欢的方式对它们进行分组 。这对于二进制或十六进制数字或者是大数来说非常方便： 123456789101112131415&gt;&gt;&gt; six_figures = 100_000&gt;&gt;&gt; six_figures100000&gt;&gt;&gt; a = 10_00_0&gt;&gt;&gt; a10000&gt;&gt;&gt; error = 0xbad_c0ffee&gt;&gt;&gt; error50159747054&gt;&gt;&gt; flags = 0b_0111_0101_0001_0101&gt;&gt;&gt; flags29973 请记住，这种改变只是语法层面上的变化，是一种在源代码中以不同方式表示数字文字的方法而已。在虚拟机编译成字节码的时候不会有任何变化，你可以在 PEP515 中了解到关于它的更多信息。 新的字符串格式化方法对字符串格式化操作有两种常用的方法，第一个是使用 “%” 操作符，第二个是使用 format 函数。 “%” 操作符 123&gt;&gt;&gt; s = &quot;%s is %d&quot; % (&apos;two&apos;, 2)&gt;&gt;&gt; s&apos;two is 2&apos; format 函数 123&gt;&gt;&gt; s = &quot;&#123;fruit&#125; is &#123;color&#125;&quot;.format(fruit=&apos;apple&apos;, color=&apos;red&apos;)&gt;&gt;&gt; s&apos;apple is red&apos; 显然，format 函数要比 % 操作符的可读性要好，在Python 3.6 增加了第三种格式化字符串方法，称为 Formatted String Literals ，简称 f字符串。 123&gt;&gt;&gt; name = &apos;Bob&apos;&gt;&gt;&gt; f&apos;Hello, &#123;name&#125;!&apos;&apos;Hello, Bob!&apos; 你还可以在字符串内使用嵌入式的 Python 表达式，例如： 1234&gt;&gt;&gt; a = 5&gt;&gt;&gt; b = 10&gt;&gt;&gt; f&apos;Five plus ten is &#123;a + b&#125; and not &#123;2 * (a + b)&#125;.&apos;&apos;Five plus ten is 15 and not 30.&apos; 这个看起来很酷，其实这种操作在模版引擎中早就有这样的特性存在，只不过因为用的人多了，就引入到了语言标准中。 除了这些，还可以操作数字 12345678910111213# 精度&gt;&gt;&gt; PI = 3.141592653&gt;&gt;&gt; f&quot;Pi is &#123;PI:.2f&#125;&quot;&gt;&gt;&gt; &apos;Pi is 3.14&apos;&gt;&gt;&gt; error = 50159747054#以16进制格式化&gt;&gt;&gt; f&apos;Programmer Error: &#123;error:#x&#125;&apos;&apos;Programmer Error: 0xbadc0ffee&apos;#以二进制格式化&gt;&gt;&gt; f&apos;Programmer Error: &#123;error:#b&#125;&apos;&apos;Programmer Error: 0b101110101101110000001111111111101110&apos; 你可以在 PEP498 中了解更多信息 变量注释 “动态语言一时爽，代码重构火葬场”，虽有危言耸听嫌疑，但的确因为动态语言的灵活性也带来代码维护困难的麻烦，我们不得不通过文档注释来对参数进行说明，而有时又因为业务需求的变更导致代码修改后没有同步文档注释造成实际代码和文档不一致的情况，如果能像静态语言一样，让程序员在语法层面就是就被限制在规则范围内做事，就不会出问题了。所以，像Java这样的语言做工程项目是有优势的。 从Python 3.5开始，可以将类型注解添加到函数和方法中： 12&gt;&gt;&gt; def my_add(a: int, b: int) -&gt; int:... return a + b 这个函数表示，a 和 b 两个参数必须是 int 类型，函数的返回值也是 int。 在语义方面没有任何改变—CPython解释器只是将类型记录为类型注释，但不做任何方式类型检查。类型检查纯粹是可选的，你需要一个像Mypy这样的工具来帮助你。 可以在PEP 526中了解更多关于这一变化的信息。 当然，这个版本不止这么一点点变化，还有： 异步生成器的语法 异步推导式语法 更快的字典结构，内存减少20％到25％ 英文原文：https://dbader.org/blog/cool-new-features-in-python-3-6]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Dictionary and Set comprehensions]]></title>
    <url>%2F2018%2F05%2F21%2Fpython-dictionary-and-set-comprehensions%2F</url>
    <content type="text"><![CDATA[大多数Python程序员都知道并使用列表推导。对于不熟悉这个概念的人来说，列表推导其实是一种更简短、更简洁的创建列表的方式。 1234&gt;&gt;&gt; some_list = [1, 2, 3, 4, 5]&gt;&gt;&gt; another_list = [ x + 1 for x in some_list ]&gt;&gt;&gt; another_list[2, 3, 4, 5, 6] 而从Python 3.1开始(也反向地移植到了Python 2.7), 我们可以用同样的方式创建集合(Set)和字典(Dict): 使用推导的方式，创建一个集合 12345&gt;&gt;&gt; # Set Comprehensions&gt;&gt;&gt; some_list = [1, 2, 3, 4, 5, 2, 5, 1, 4, 8]&gt;&gt;&gt; even_set = &#123; x for x in some_list if x % 2 == 0 &#125;&gt;&gt;&gt; even_setset([8, 2, 4]) 使用推导的方式，创建一个Dict 1234&gt;&gt;&gt; # Dict Comprehensions&gt;&gt;&gt; d = &#123; x: x % 2 == 0 for x in range(1, 11) &#125;&gt;&gt;&gt; d&#123;1: False, 2: True, 3: False, 4: True, 5: False, 6: True, 7: False, 8: True, 9: False, 10: True&#125; 另一个创建集合的方式: 123&gt;&gt;&gt; my_set = &#123;1, 2, 1, 2, 3, 4&#125;&gt;&gt;&gt; my_setset([1, 2, 3, 4]) 没有使用到内建的 set方法, 直接用大括号{}创建。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 元类(Metaclasses)]]></title>
    <url>%2F2018%2F05%2F19%2Fpython-metaclasses%2F</url>
    <content type="text"><![CDATA[Python 元类(Metaclasses)1、Type and Class在Python3中，所有类都是新式类(New-Style)。因此，类的类型和它的实例类型是可以互换的。在Python中，所有的东西都是object。类也是一个object。因此类一定有一个type。例如： 1234567&gt;&gt;&gt; class Foo: pass...&gt;&gt;&gt; x = Foo()&gt;&gt;&gt; type(x)&lt;class '__main__.Foo'&gt;&gt;&gt;&gt; type(Foo)&lt;class 'type'&gt; 如上所示：x的类型是Foo，但是Foo的类型，类的本身就是type。我们熟悉的内置类的类型也是type: 12345678&gt;&gt;&gt; for t in int, float, dict, list, tuple:... print(type(t))...&lt;class 'type'&gt;&lt;class 'type'&gt;&lt;class 'type'&gt;&lt;class 'type'&gt;&lt;class 'type'&gt; type本身的类型也是type 123&gt;&gt;&gt; type(type)&lt;class 'type'&gt;&gt;&gt;&gt; type是一个元类，其中类是实例。就像普通对象是类的实例，Python中的任何新式类，以及Python 3中的任何类，都是类的元类实例。 还是上述的例子： 1234567&gt;&gt;&gt; class Foo: pass...&gt;&gt;&gt; x = Foo()&gt;&gt;&gt; type(x)&lt;class '__main__.Foo'&gt;&gt;&gt;&gt; type(Foo)&lt;class 'type'&gt; x是类Foo的实例 Foo 是type元类的实例 type也是元类的一个实例，因此它是它自己的一个实例。 2、如何动态创建类内置类型()函数在传递一个参数时，返回对象的类型。对于新式类，通常与对象的class属性相同: 123456789101112131415&gt;&gt;&gt; type(3)&lt;class 'int'&gt;&gt;&gt;&gt; type(['foo', 'bar', 'baz'])&lt;class 'list'&gt;&gt;&gt;&gt; t = (1, 2, 3, 4, 5)&gt;&gt;&gt; type(t)&lt;class 'tuple'&gt;&gt;&gt;&gt; class Foo:... pass...&gt;&gt;&gt; type(Foo())&lt;class '__main__.Foo'&gt; 我们还可以使用三个参数的type(， ， )方法来动态创建一个新类: 指定了类名。这成为该类的name属性。 指定了类继承的基类的一个元组。这成为该类的bases属性。 指定包含类主体定义的名称空间词典。这成为该类的dict属性。以这种方式调用类型()将创建类型元类的一个新实例。换句话说，它动态创建一个新类。 Example 1 最基本的使用1234&gt;&gt;&gt; Foo = type('Foo', (), &#123;&#125;)&gt;&gt;&gt; x = Foo()&gt;&gt;&gt; x&lt;__main__.Foo object at 0x102b563c8&gt; 上述通过type()动态创建类和下面代码创建类是一样的。 123456&gt;&gt;&gt; class Foo:... pass...&gt;&gt;&gt; x = Foo()&gt;&gt;&gt; x&lt;__main__.Foo object at 0x102b56400&gt; 最佳实战：动态创建类有一个很大好处。比如当你数据库表是采用分表设计。如： 12345table_1table_2table_3...table_N 这个时候你要根据表来创建对应的模型。你会发现table结构是一致，只是表名不同。那么最好的方式就是通过type()元类动态创建模型type(‘table_’+id, (), {})。 Example 2 继承123456789&gt;&gt;&gt; Bar = type('Bar', (Foo,), dict(attr=100))&gt;&gt;&gt; x = Bar()&gt;&gt;&gt; x.attr100&gt;&gt;&gt; x.__class__&lt;class '__main__.Bar'&gt;&gt;&gt;&gt; x.__class__.__bases__(&lt;class '__main__.Foo'&gt;,) 1234567891011&gt;&gt;&gt; class Bar(Foo):... attr = 100...&gt;&gt;&gt; x = Bar()&gt;&gt;&gt; x.attr100&gt;&gt;&gt; x.__class__&lt;class '__main__.Bar'&gt;&gt;&gt;&gt; x.__class__.__bases__(&lt;class '__main__.Foo'&gt;,) Example 3 默认属性1234567891011121314&gt;&gt;&gt; Foo = type(... 'Foo',... (),... &#123;... 'attr': 100,... 'attr_val': lambda x : x.attr... &#125;... )&gt;&gt;&gt; x = Foo()&gt;&gt;&gt; x.attr100&gt;&gt;&gt; x.attr_val()100 1234567891011&gt;&gt;&gt; class Foo:... attr = 100... def attr_val(self):... return self.attr...&gt;&gt;&gt; x = Foo()&gt;&gt;&gt; x.attr100&gt;&gt;&gt; x.attr_val()100 Examle 4 默认属性引用外部函数1234567891011121314151617&gt;&gt;&gt; def f(obj):... print('attr =', obj.attr)...&gt;&gt;&gt; Foo = type(... 'Foo',... (),... &#123;... 'attr': 100,... 'attr_val': f... &#125;... )&gt;&gt;&gt; x = Foo()&gt;&gt;&gt; x.attr100&gt;&gt;&gt; x.attr_val()attr = 100 12345678910111213&gt;&gt;&gt; def f(obj):... print('attr =', obj.attr)...&gt;&gt;&gt; class Foo:... attr = 100... attr_val = f...&gt;&gt;&gt; x = Foo()&gt;&gt;&gt; x.attr100&gt;&gt;&gt; x.attr_val()attr = 100 参考文献1、https://realpython.com/python-metaclasses/#type-and-class]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Old-Style Vs. New-Style Classes]]></title>
    <url>%2F2018%2F05%2F18%2Fpython-old-style-vs-new-style-classes%2F</url>
    <content type="text"><![CDATA[Python Old-Style Vs. New-Style Classes在Python中，类有两种方式定义，一种是非正式的，我们称为旧式的或者叫经典的(Old-Style)，另一种是正式的(New-Style)。 换一种说法，对于继承自object的类，则称为New-Style类。未继承object的类称为Old-Style类。 在Python2.7中New-Style和Old-Style类存在一些区别。 Old-Style Classes对于Old-Style类的实例，它的类和类型并不相同。Old-Style类的实例是一个内置类型，而类型是一个实例。例如：obj是一个Old-Style的实例。那么obj.class是一个内置类，而type(obj)是一个实例。 1234567&gt;&gt;&gt; class Foo: pass...&gt;&gt;&gt; obj = Foo()&gt;&gt;&gt; obj.__class__&lt;class __main__.Foo at 0x103faba10&gt;&gt;&gt;&gt; type(obj)&lt;type 'instance'&gt; New-Style ClassesNew-Style类统一了类和类型的概念。例如：obj是一个New-Style的实例，obj.class和type(obj)相同: 1234567&gt;&gt;&gt; class Foo(object): pass...&gt;&gt;&gt; obj = Foo()&gt;&gt;&gt; obj.__class__&lt;class '__main__.Foo'&gt;&gt;&gt;&gt; type(obj)&lt;class '__main__.Foo'&gt; 123456789&gt;&gt;&gt; n = 10&gt;&gt;&gt; d = &#123;&apos;x&apos;: 1, &apos;y&apos;: 2&#125;&gt;&gt;&gt; x = Foo()&gt;&gt;&gt; for obj in (n, d, x):... print(type(obj) is obj.__class__)...TrueTrueTrue 在 Python 2.7中New-Style类和Old-Style类在多继承方面也会有差异： Old-Style Classes1234567891011121314151617class A: def foo(self): print('called A.foo()')class B(A): passclass C(A): def foo(self): print('called C.foo()')class D(B, C): passif __name__ == '__main__': d = D() d.foo() 1Out: called A.foo() New-Style Classes1234567891011121314151617class A(object): def foo(self): print('called A.foo()')class B(A): passclass C(A): def foo(self): print('called C.foo()')class D(B, C): passif __name__ == '__main__': d = D() d.foo() 1Out: called C.foo() B、C 是 A 的子类，D 多继承了 B、C 两个类，其中 C 重写了 A 中的 foo() 方法。 如果 A 是经典类，当调用 D 的实例的 foo() 方法时，Python 会按照深度优先的方法去搜索 foo()，路径是 B-A-C ，执行的是 A 中的 foo() ； 如果 A 是新式类，当调用 D 的实例的 foo() 方法时，Python 会按照广度优先的方法去搜索 foo()，路径是 B-C-A ，执行的是 C 中的 foo() 。 因为 D 是直接继承 C 的，从逻辑上说，执行 C 中的 foo() 更加合理，因此新式类对多继承的处理更为合乎逻辑。 注意： 在 Python 3.x 中的新式类貌似已经兼容了经典类，无论 A 是否继承 object 类， D 实例中的 foo() 都会执行 C 中的 foo() 。但是在 Python 2.7 中这种差异仍然存在，因此还是推荐使用新式类，要继承 object 类。 参考文献1、https://realpython.com/python-metaclasses/#old-style-vs-new-style-classes 2、https://www.zhihu.com/question/19754936/answer/202650790]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python类的__dict__无法更新]]></title>
    <url>%2F2018%2F05%2F17%2Fpython-class-dict-cannot-update%2F</url>
    <content type="text"><![CDATA[Python类的dict无法更新？测试环境: Python2.7 123456&gt;&gt;&gt; class O(object): pass...&gt;&gt;&gt; O.__dict__['name'] = 'Finger'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'dictproxy' object does not support item assignment 答：是的, class的__dict__是只读的 12345&gt;&gt;&gt; O.__dict__dict_proxy(&#123;'__dict__': &lt;attribute '__dict__' of 'O' objects&gt;, '__module__': '__main__', '__weakref__': &lt;attribute '__weakref__' of 'O' objects&gt;, '__doc__': None&#125;)&gt;&gt;&gt; O.__dict__.items()[('__dict__', &lt;attribute '__dict__' of 'O' objects&gt;), ('__module__', '__main__'), ('__weakref__', &lt;attribute '__weakref__' of 'O' objects&gt;), ('__doc__', None)] 可以看到O.__dict__ 是一个dictproxy对象，而不是一个dict。 12&gt;&gt;&gt; dir(O.__dict__)['__class__', '__cmp__', '__contains__', '__delattr__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'copy', 'get', 'has_key', 'items', 'iteritems', 'iterkeys', 'itervalues', 'keys', 'values'] 我们dir(O.__dict__)，发现它没有__setitem__方法 那么我们怎么改类设置属性呢？可以用setattr 1234&gt;&gt;&gt; setattr(O, 'name', 'Finger')&gt;&gt;&gt; O.name'Finger'&gt;&gt;&gt; 可能在测试的过程中你会发现，如果你采用Python经典继承(Old-Style Classes)看到的结果却是这样的： 12345&gt;&gt;&gt; class O: pass...&gt;&gt;&gt; O.__dict__['name'] = 'Finger'&gt;&gt;&gt; O.name'Finger' 关于Python的Old-Style和New-Style Classes下次再探讨 注意：在Python3.x中不管是Old-Style还是New-Style Classes都是不允许对类的__dict__进行修改。因为不管哪种继承方式，它们都是mappingproxy对象 123456789101112&gt;&gt;&gt; class O: pass...&gt;&gt;&gt; O.__dict__['name'] = 'Finger'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'mappingproxy' object does not support item assignment&gt;&gt;&gt; class O(object): pass...&gt;&gt;&gt; O.__dict__['name'] = 'Finger'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'mappingproxy' object does not support item assignment]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Python实现Elasticsearch function_score查询(二)]]></title>
    <url>%2F2017%2F08%2F26%2Fusing-elasticsearch-function-score-with-python-2%2F</url>
    <content type="text"><![CDATA[random_scorerandom_score函数，它的输出是一个介于0到1之间的数字，当给它提供相同的seed值时，它能够产生一致性随机的结果。random_score 子句不包含任何的filter，因此它适用于所有文档。当然，如果你索引了能匹配查询的新文档，无论你是否使用了一致性随机，结果的顺序都会有所改变。 123456q = query.Q( 'function_score', functions=[ query.SF('random_score', seed=10) ]) 12345678910GET /_search&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;random_score&quot;: &#123; &quot;seed&quot;: 10 &#125; &#125; &#125;&#125; decay functions之前提过的weight，field_value_factor，random_score等函数都是一种线性的影响关系，给出的分数随着距离变大而变低。有时候我们会有范围性质，分数变化缓慢的需求场景。此时上述函数就显得不那么适用。Elasticsearch提供了三种decay functions，让我们可以基于一个单值的数值型字段（比如日期、地点或者价格类似标准的数值型字段）来计算分数。 这三个衰减函数分别为： 线性衰减(linear) 指数衰减(exp) 高斯衰减(gauss) 它们可以操作数值，时间以及经纬度地理坐标点这样的字段，这三个函数都有以下参数： origin: 中心点或字段可能的最佳值，落在原点origin上的文档评分 _score 为满分1.0 。 scale: 衰减率，即一个文档从原点origin下落时，评分_score改变的速度(例如每 £10 欧元或每 100 米)。 decay: 从原点origin衰减到scale所得的评分_score，默认值为 0.5。 offset: 以原点origin为中心点，为其设置一个非零的偏移量offset 覆盖一个范围，而不只是单个原点。在范围 -offset &lt;= origin &lt;= +offset 内的所有评分_score都是 1.0。 origin和scale参数是必须的。origin是中心点，计算是从这点开始。scale是衰变率。默认情况下，offset参数设置为0；如果定义了该参数，decay函数将只计算文档值比此参数的值大的文档得分。decay参数告诉Elasticsearch应该降低多少分数，默认设置为0.5。 下面我们举一个例子：假设用户希望租一个离伦敦市中心({‘lat’:51.50, ‘lon’:0.12})且每晚不超过 £100 英镑的度假屋而且与距离相比，用户对价格更为敏感，这样查询可以写成：123456789q = query.Q( 'function_score', query=query.Q('match', properties="balcony"), functions=[ query.SF('gauss', price=&#123;'origin': '0', 'scale': '20'&#125;), query.SF('gauss', location=&#123;'origin': '51.50, 0.12', 'scale': '2km'&#125;) ], score_mode="multiply") 123456789101112131415161718192021222324252627282930&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;functions&quot;: [ &#123; &quot;gauss&quot;: &#123; &quot;price&quot;: &#123; &quot;origin&quot;: &quot;0&quot;, &quot;scale&quot;: &quot;20&quot; &#125; &#125; &#125;, &#123; &quot;gauss&quot;: &#123; &quot;location&quot;: &#123; &quot;origin&quot;: &quot;11, 12&quot;, &quot;scale&quot;: &quot;2km&quot; &#125; &#125; &#125; ], &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;properties&quot;: &quot;balcony&quot; &#125; &#125;, &quot;score_mode&quot;: &quot;multiply&quot; &#125; &#125; &#125; 关于decay functions的衰减曲线性：如下图： 图中所有曲线的原点origin（即中心点）的值都是40，offset是5，也就是在范围 40 - 5 &lt;= value &lt;= 40 + 5 内的所有值都会被当作原点origin处理所有这些点的评分都是满分1.0 。在此范围之外，评分开始衰减，衰减率由scale值（此例中的值为 5 ）和 衰减值decay此例中为默认值 0.5 ）共同决定。结果是所有三个曲线在 origin +/- (offset + scale) 处的评分都是 0.5，即点30和50处。 linear 、 exp 和 gauss函数三者之间的区别在于范围（ origin +/- (offset + scale) ）之外的曲线形状： linear 线性函数是条直线，一旦直线与横轴 0 相交，所有其他值的评分都是 0.0 。 exp 指数函数是先剧烈衰减然后变缓。 gauss 高斯函数是钟形的——它的衰减速率是先缓慢，然后变快，最后又放缓。 选择曲线的依据完全由期望评分_score的衰减速率来决定，即距原点origin的值。 Demo地址：https://github.com/zhoujun/mydemos/tree/master/elasticsearch-demo]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Python实现Elasticsearch function_score查询(一)]]></title>
    <url>%2F2017%2F08%2F19%2Fusing-elasticsearch-function-score-with-python-1%2F</url>
    <content type="text"><![CDATA[什么是function_scorefunction_score查询是处理分值计算过程的终极工具。它让你能够对所有匹配了主查询的每份文档调用一个函数来调整甚至是完全替换原来的_score。 实际上，你可以通过设置过滤器来将查询得到的结果分成若干个子集，然后对每个子集使用不同的函数。这样你就能够同时得益于：高效的分值计算以及可以缓存的过滤器。 它拥有几种预先定义好了的函数： weight 对每份文档适用一个简单的提升，且该提升不会被归约：当weight为2时，结果为2 * _score。 field_value_factor 使用文档中某个字段的值来改变_score，比如将受欢迎程度或者投票数量考虑在内。 random_score 使用一致性随机分值计算来对每个用户采用不同的结果排序方式，对相同用户仍然使用相同的排序方式。 衰减函数(Decay Function)-(linear，exp，gauss) 将像publish_date，geo_location或者price这类浮动值考虑到_score中，偏好最近发布的文档，邻近于某个地理位置(译注：其中的某个字段)的文档或者价格(译注：其中的某个字段)靠近某一点的文档。 script_score 使用自定义的脚本来完全控制分值计算逻辑。如果你需要以上预定义函数之外的功能，可以根据需要通过脚本进行实现。 没有function_score查询的话，我们也许就不能将全文搜索得到分值和近因进行结合了。我们将不得不根据_score或者date进行排序；无论采用哪一种都会抹去另一种的影响。function_score查询让我们能够将两者融合在一起：仍然通过全文相关度排序，但是给新近发布的文档，或者流行的文档，或者符合用户价格期望的文档额外的权重。你可以想象，一个拥有所有这些功能的查询看起来会相当复杂。我们从一个简单的例子开始，循序渐进地对它进行介绍。 如何使用function_score假设我们有一个博客网站让用户投票选择他们喜欢的文章。我们希望让人气高的文章出现在结果列表的头部，但是主要的排序依据仍然是全文搜索分值。我们可以通过保存每篇文章的投票数量来实现。 123456789101112131415161718192021222324252627282930313233# 1、新建一个索引映射：class Post(DocType): title = String() content = Text() votes = Integer() created_at = Date() class Meta: index = 'blogposts' type = 'post'Post.init()# 2、添加测试数据def add_post(id_, title, content): post = Post(meta=&#123;'id': id_&#125;) post.title = title post.content = content post.votes = random.randint(0, 100) post.save()# 3、初始化数据def init_data(): add_post(1, 'Python is good', 'Python is good') add_post(2, 'Python is beautiful', 'Python is beautiful') add_post(3, 'Python is nice', 'Python is nice') add_post(4, 'Today is good day', 'Today is good day') add_post(5, 'Today is nice day', 'Today is nice day') add_post(6, 'The key aspect in promoting', 'The key aspect in promoting') add_post(7, 'Just a month after we started', 'Just a month after we started working on reddit') add_post(8, "If I can't hear your heartbeat", "If I can't hear your heartbeat, you're too far away.") add_post(9, 'No Country for Old Men', 'No Country for Old Men') add_post(10, 'Django is web framework', 'Django is python lib') field_value_factor使用field_value_factor函数的function_score查询将投票数和全文相关度分值结合起来：1234567891011121314151617181920q = query.Q( 'function_score', query=query.Q("multi_match", query='python', fields=['title', 'content']), functions=[ query.SF('field_value_factor', field='votes') ])s = Post.search()s = s.query(q)response = s.execute()for h in response: print(h.title)"""Out:Python is nicePython is goodPython is beautifulDjango is web framework""" function_score查询会包含主查询(Main Query)和希望使用的函数。先会执行主查询，然后再为匹配的文档调用相应的函数。每份文档中都必须有一个votes字段用来保证function_score能够起作用。 在前面的例子中，每份文档的最终_score会通过下面的方式改变： new_score = old_score * number_of_votes 它得到的结果并不好。全文搜索的_score通常会在0到10之间。而从下图我们可以发现，拥有10票以上的文章的分值大大超过了这个范围，而没有被投票的文章的分值会被重置为0。 modifier为了让votes值对最终分值的影响更缓和，我们可以使用modifier。换言之，我们需要让头几票的效果更明显，其后的票的影响逐渐减小。0票和1票的区别应该比10票和11票的区别要大的多。 一个用于此场景的典型modifier是log1p，它将公式改成这样： new_score = old_score * log(1 + number_of_votes) log函数将votes字段的效果减缓了，其效果类似下面的曲线：使用了modifier参数的请求如下：1234567q = query.Q( 'function_score', query=query.Q("multi_match", query='python', fields=['title', 'content']), functions=[ query.SF('field_value_factor', field='votes', modifier='log1p') ]) 可用的modifiers有：none(默认值)，log，log1p，log2p，ln，ln1p，ln2p，square，sqrt以及reciprocal。 Modifier Meaning none Do not apply any multiplier to the field value log Take the logarithm of the field value log1p Add 1 to the field value and take the logarithm log2p Add 2 to the field value and take the logarithm ln Take the natural logarithm of the field value ln1p Add 1 to the field value and take the natural logarithm ln2p Add 2 to the field value and take the natural logarithm square Square the field value (multiply it by itself) sqrt Take the square root of the field value reciprocal Reciprocate the field value, same as 1/x where x is the field’s value 具体详情请查阅官网介绍 factor可以通过将votes字段的值乘以某个数值来增加该字段的影响力，这个数值被称为factor：1234567q = query.Q( 'function_score', query=query.Q("multi_match", query='python', fields=['title', 'content']), functions=[ query.SF('field_value_factor', field='votes', modifier='log1p', factor=2) ]) 添加了factor将公式修改成这样： new_score = old_score log(1 + factor number_of_votes)当factor大于1时，会增加其影响力，而小于1的factor则相应减小了其影响力，如下图所示： boost_mode将全文搜索的相关度分值乘以field_value_factor函数的结果，对最终分值的影响可能太大了。通过boost_mode参数，我们可以控制函数的结果应该如何与_score结合在一起，该参数接受下面的值： 模型 描述 multiply _score 与函数值的积（默认） sum _score 与函数值的和 avg _score 平均值 first 应用filter匹配上的第一个函数值 min _score 与函数值间的较小值 max _score 与函数值间的较大值 replace 将_score替换成函数结果 如果我们是通过将函数结果累加来得到_score，其影响会小的多，特别是当我们使用了一个较低的factor时：12345678q = query.Q( 'function_score', query=query.Q("multi_match", query='python', fields=['title', 'content']), functions=[ query.SF('field_value_factor', field='votes', modifier='log1p', factor=0.1) ], score_mode="sum") 上述请求的公式如下所示： new_score = old_score + log(1 + 0.1 * number_of_votes) max_boost最后，我们能够通过制定max_boost参数来限制函数的最大影响：123456789q = query.Q( 'function_score', query=query.Q("multi_match", query='python', fields=['title', 'content']), functions=[ query.SF('field_value_factor', field='votes', modifier='log1p', factor=0.1) ], score_mode="sum", max_boost=1.5) 无论field_value_factor函数的结果是多少，它绝不会大于1.5。 NOTE max_boost只是对函数的结果有所限制，并不是最终的_score。 参考文档https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html http://itindex.net/detail/57059-elasticsearch-%E6%8E%A7%E5%88%B6-%E7%9B%B8%E5%85%B3]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Python实现Elasticsearch提供的搜索建议]]></title>
    <url>%2F2017%2F08%2F16%2Felasticsearch-search-suggest-with-python%2F</url>
    <content type="text"><![CDATA[Suggest搜索Elasticsearch支持多种suggest类型，也支持模糊搜索。 Elasticsearch支持如下4种搜索类型： Term。基于编辑距离的搜索，也就是对比两个字串之间，由一个转成另一个所需的最少编辑操作次数，编辑距离越少说明越相近。搜索时需要指定字段，是很基础的一种搜索。 Phrase。Term的优化，能够基于共现和频率来做出关于选择哪些token的更好的决定。 Completion。提供自动完成/按需搜索的功能，这是一种导航功能，可在用户输入时引导用户查看相关结果，从而提高搜索精度。和前2种用法不同，需要在mapping时指定suggester字段，使用允许快速查找的数据结构，所以在搜索速度上得到了很大的优化。 Context。Completion搜索的是索引中的全部文档，但是有时候希望对这个结果进行一些and/or的过滤，就需要使用Context类型了。 下面使用elasticsearch_dsl自带的Suggestions功能1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#!/usr/bin/env python# -*- coding: utf-8 -*-from elasticsearch_dsl import DocType, Text, Keyword, Completionfrom elasticsearch_dsl.connections import connectionsconnections.create_connection(hosts=['localhost'])class BlogLive(DocType): subject = Text() description = Text() topics = Keyword() live_suggest = Completion() class Meta: index = 'blog' type = 'live' def save(self, ** kwargs): return super(BlogLive, self).save(** kwargs)# create the mappings in elasticsearchBlogLive.init()def add_live(id_, subject, description): live = BlogLive(meta=&#123;'id': id_&#125;) live.subject = subject live.description = description live.live_suggest = subject live.save()def init_data(): add_live(1, 'python', 'python is good') add_live(2, 'python django', 'django is web framework') add_live(3, 'python flask', 'flask is web framework') add_live(4, 'elasticsearch', 'elasticsearch is searchengine') add_live(5, 'elasticsearch-dsl python', 'python elasticsearch dsl is client API')def suggest(key): s = BlogLive.search() s = s.suggest('live_suggestion', key, completion=&#123;'field': 'live_suggest', 'fuzzy': &#123;'fuzziness': 2, 'prefix_length': 2&#125;, 'size': 10&#125;) print(s.to_dict()) """ &#123;'suggest': &#123;'live_suggestion': &#123; 'completion': &#123;'field': 'live_suggest', 'size': 10, 'fuzzy': &#123;'fuzziness': 2, 'prefix_length': 2&#125;&#125;, 'text': 'python' &#125; &#125; &#125; """ suggestions = s.execute_suggest() # print(suggestions.live_suggestion) for match in suggestions.live_suggestion[0].options: source = match._source print(source['subject'], source['description'], match._score)if __name__ == '__main__': init_data() suggest('python')]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>SearchEngine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using Elasticsearch Bulk With Python]]></title>
    <url>%2F2017%2F08%2F16%2Fusing-elasticsearch-bulk-with-python%2F</url>
    <content type="text"><![CDATA[Python Elasticsearch客户端提供了批量索引的功能，详情请查看项目主页 主要封装了两种方式进行bulk操作： streaming_bulk 流式 parallel_buck 多线程 123456789101112131415161718192021222324def streaming_bulk( client, actions, chunk_size=500, max_chunk_bytes=104857600, raise_on_error=True, expand_action_callback=&lt;function expand_action&gt;, raise_on_exception=True, max_retries=0, initial_backoff=2, max_backoff=600, yield_ok=True, **kwargs): passdef parallel_bulk( client, actions, thread_count=4, chunk_size=500, max_chunk_bytes=104857600, queue_size=4, expand_action_callback=&lt;function expand_action&gt;, **kwargs): pass streaming比parallel多了重试的机制。两个方法默认都是每次处理500索引，两个方法都返回迭代器对象，bulk方法默认使用的是streaming_bulk方式。如果要使用parallel_buck需要修改源代码或者自己扩展。 bulk方法定义：12345678910111213141516def bulk(client, actions, stats_only=False, **kwargs): success, failed = 0, 0 # list of errors to be collected is not stats_only errors = [] for ok, item in streaming_bulk(client, actions, **kwargs): # go through request-reponse pairs and detect failures if not ok: if not stats_only: errors.append(item) failed += 1 else: success += 1 return success, failed if stats_only else errors client 为ElasticSearch实例对象 actions 索引内容 stats_only 为True则只返回成功和失败的数量，False则会返回具体的失败内容列表 actions 内容为：123456789101112&#123; &apos;_op_type&apos;: &apos;delete&apos;, &apos;_index&apos;: &apos;index-name&apos;, &apos;_type&apos;: &apos;type-name&apos;, &apos;_id&apos;: 1,&#125;&#123; &apos;_op_type&apos;: &apos;update&apos;, &apos;_index&apos;: &apos;index-name&apos;, &apos;_type&apos;: &apos;type-name&apos;, &apos;_id&apos;: 1,&#125; _op_type 可以是index, create, delete, update操作，默认使用index 12345678910111213141516171819202122232425262728293031323334353637383940#!/usr/bin/env python# -*- coding: utf-8 -*-from elasticsearch import Elasticsearchfrom elasticsearch import helpersdef test_streaming_bulk(): actions = [] for i in xrange(0, 100000): actions.append(&#123;'_index': 'test_index', '_type': 'test', 'x': i&#125;) helpers.bulk(es, actions, False)def test_parallel_bulk(): actions = [] for i in xrange(0, 100000): actions.append(&#123;'_index': 'test_index', '_type': 'test', 'x': i&#125;) parallel_bulk(es, actions)def parallel_bulk(client, actions, stats_only=False, **kwargs): success, failed = 0, 0 # list of errors to be collected is not stats_only errors = [] for ok, item in helpers.parallel_bulk(client, actions, **kwargs): # print ok, item # go through request-reponse pairs and detect failures if not ok: if not stats_only: errors.append(item) failed += 1 else: success += 1 return success, failed if stats_only else errors 测试代码： https://github.com/zhoujun/mydemos/tree/master/elasticsearch-demo]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>SearchEngine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch Documents APIs]]></title>
    <url>%2F2017%2F08%2F15%2Felasticsearch-document-api%2F</url>
    <content type="text"><![CDATA[一、Elasticsearch 索引别名索引别名就像一个快捷方式或软链接，可以指向一个或多个索引，也可以给任何需要索引名的API使用。别名带给我们极大的灵活性，允许我们做到： 在一个运行的集群上无缝的从一个索引切换到另一个 给多个索引分类 给索引的一个子集创建视图 1234567891011# 创建别名PUT /my_index_v1 # 将别名my_index指向my_index_v1PUT /my_index_v1/_alias/my_index # 检查别名指向哪个索引GET /*/_alias/my_index# 或者哪个别名指向这个索引GET /my_index_v1/_alias/* 如果我们决定要修改索引中一个字段的映射。但是我们不能修改现存的映射，因此我们需要重新索引数据。这个时候别名就可以帮助我们完成。 首先，我们需要创建新的索引 my_index_v21PUT /my_index_v2 然后我们将数据从 my_index_v1 迁移到 my_index_v21234567POST /_aliases&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;my_index_v1&quot;, &quot;alias&quot;: &quot;my_index&quot; &#125;&#125;, &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;my_index_v2&quot;, &quot;alias&quot;: &quot;my_index&quot; &#125;&#125; ]&#125; 最后，我们的应用就从旧的索引迁移到了新的，而没有停机时间。 提示： 即使你认为现在的索引设计已经是完美的了，当你的应用在生产环境使用时，还是有可能在今后有一些改变的。索引请做好准备：在应用中使用别名而不是索引。然后你就可以在任何时候重新索引。别名的开销很小，应当广泛使用。 二、Elasticsearch 文档APIs1、创建文档12345678910111213141516171819202122# 创建index为twitter, type为tweet，id为1的文档PUT twitter/tweet/1&#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125;# 结果为：&#123; &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;failed&quot; : 0, &quot;successful&quot; : 2 &#125;, &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;tweet&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;created&quot; : true, &quot;result&quot; : created&#125; 2、获取文档123456789101112131415GET twitter/tweet/1&#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;tweet&quot;, &quot;_id&quot; : &quot;0&quot;, &quot;_version&quot; : 1, &quot;found&quot;: true, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;likes&quot;: 0, &quot;message&quot; : &quot;trying out Elasticsearch&quot; &#125;&#125; 3、删除文档123456789101112131415DELETE /twitter/tweet/1&#123; &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;failed&quot; : 0, &quot;successful&quot; : 2 &#125;, &quot;found&quot; : true, &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;tweet&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 2, &quot;result&quot;: &quot;deleted&quot;&#125; 4、删除Type映射报错：No handler found for uri [/edemo/test] and method [DELETE] 因为ElasticSearch已经不支持删除一个type了。所以如果想要删除type有两种选择： 重新设置index 删除type下的所有内容 1、如果是重新设置index的话，官方建议尽可能不要直接删除type。应该是删除index然后重新创建type。具体请查看官方文档 2、如果是想要删除type下的所有数据的话，可以使用delete by query的方法:123456POST index/type/_delete_by_query?conflicts=proceed&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 具体请查看官方文档 5、查看映射(Mapping)12345# 查看index中的所有mappingGET /index/_mapping# 查看index中某一个type的mappingGET /index/_mapping/type 6、批量操作(Bulk)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980POST _bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_id&quot; : &quot;1&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value1&quot; &#125;&#123; &quot;delete&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_id&quot; : &quot;2&quot; &#125; &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_id&quot; : &quot;3&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value3&quot; &#125;&#123; &quot;update&quot; : &#123;&quot;_id&quot; : &quot;1&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_index&quot; : &quot;test&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;field2&quot; : &quot;value2&quot;&#125; &#125;# 结果为：&#123; &quot;took&quot;: 30, &quot;errors&quot;: false, &quot;items&quot;: [ &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;test&quot;, &quot;_type&quot;: &quot;type1&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true, &quot;status&quot;: 201 &#125; &#125;, &#123; &quot;delete&quot;: &#123; &quot;found&quot;: false, &quot;_index&quot;: &quot;test&quot;, &quot;_type&quot;: &quot;type1&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;not_found&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;status&quot;: 404 &#125; &#125;, &#123; &quot;create&quot;: &#123; &quot;_index&quot;: &quot;test&quot;, &quot;_type&quot;: &quot;type1&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true, &quot;status&quot;: 201 &#125; &#125;, &#123; &quot;update&quot;: &#123; &quot;_index&quot;: &quot;test&quot;, &quot;_type&quot;: &quot;type1&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;status&quot;: 200 &#125; &#125; ]&#125; 7、其他 Update API Update By Query API Multi Get API Reindex API 参考文档https://www.elastic.co/guide/en/elasticsearch/reference/current/docs.html http://blog.csdn.net/leafage_m/article/details/74011357]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>SearchEngine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Elasticsearch DSL 使用笔记(二)]]></title>
    <url>%2F2017%2F08%2F13%2Felasticsearch-dsl-with-python-usage-2%2F</url>
    <content type="text"><![CDATA[使用ElasticSearch DSL进行搜索Search主要包括： 查询(queries) 过滤器(filters) 聚合(aggreations) 排序(sort) 分页(pagination) 额外的参数(additional parameters) 相关性(associated) 创建一个查询对象123456from elasticsearch import Elasticsearchfrom elasticsearch_dsl import Searchclient = Elasticsearch()s = Search(using=client) 初始化测试数据12345678910111213def add_article(id_, title, body, tags): article = Article(meta=&#123;'id': id_&#125;, title=title, tags=tags) article.body = body article.published_from = datetime.now() article.save()def init_test_data(): add_article(2, 'Python is good!', 'Python is good!', ['python']) add_article(3, 'Elasticsearch', 'Distributed, open source search and analytics engine', ['elasticsearch']) add_article(4, 'Python very quickly', 'Python very quickly', ['python']) add_article(5, 'Django', 'Python Web framework', ['python', 'django']) 第一个查询语句1234567891011121314151617181920# 创建一个查询语句s = Search().using(client).query("match", title="python")# 查看查询语句对应的字典结构print(s.to_dict())# &#123;'query': &#123;'match': &#123;'title': 'python'&#125;&#125;&#125;# 发送查询请求到Elasticsearchresponse = s.execute()# 打印查询结果for hit in s: print(hit.title)# Out:Python is good!Python very quickly# 删除查询s.delete() 1、Queries123456789101112131415161718192021222324252627282930313233343536# 创建一个多字段查询multi_match = MultiMatch(query='python', fields=['title', 'body'])s = Search().query(multi_match)print(s.to_dict())# &#123;'query': &#123;'multi_match': &#123;'fields': ['title', 'body'], 'query': 'python'&#125;&#125;&#125;# 使用Q语句q = Q("multi_match", query='python', fields=['title', 'body'])# 或者q = Q(&#123;"multi_match": &#123;"query": "python", "fields": ["title", "body"]&#125;&#125;)s = Search().query(q)print(s.to_dict())# If you already have a query object, or a dict # representing one, you can just override the query used # in the Search object:s.query = Q('bool', must=[Q('match', title='python'), Q('match', body='best')])print(s.to_dict())# 查询组合q = Q("match", title='python') | Q("match", title='django')s = Search().query(q)print(s.to_dict())# &#123;"bool": &#123;"should": [...]&#125;&#125;q = Q("match", title='python') &amp; Q("match", title='django')s = Search().query(q)print(s.to_dict())# &#123;"bool": &#123;"must": [...]&#125;&#125;q = ~Q("match", title="python")s = Search().query(q)print(s.to_dict())# &#123;"bool": &#123;"must_not": [...]&#125;&#125; 2、Filters1234567891011121314s = Search()s = s.filter('terms', tags=['search', 'python'])print(s.to_dict())# &#123;'query': &#123;'bool': &#123;'filter': [&#123;'terms': &#123;'tags': ['search', 'python']&#125;&#125;]&#125;&#125;&#125;s = s.query('bool', filter=[Q('terms', tags=['search', 'python'])])print(s.to_dict())# &#123;'query': &#123;'bool': &#123;'filter': [&#123;'terms': &#123;'tags': ['search', 'python']&#125;&#125;]&#125;&#125;&#125;s = s.exclude('terms', tags=['search', 'python'])# 或者s = s.query('bool', filter=[~Q('terms', tags=['search', 'python'])])print(s.to_dict())# &#123;'query': &#123;'bool': &#123;'filter': [&#123;'bool': &#123;'must_not': [&#123;'terms': &#123;'tags': ['search', 'python']&#125;&#125;]&#125;&#125;]&#125;&#125;&#125; 3、Aggregations1234567891011121314151617181920212223242526272829303132333435s = Search()a = A('terms', filed='title')s.aggs.bucket('title_terms', a)print(s.to_dict())# &#123;# 'query': &#123;# 'match_all': &#123;&#125;# &#125;,# 'aggs': &#123;# 'title_terms': &#123;# 'terms': &#123;'filed': 'title'&#125;# &#125;# &#125;# &#125;# 或者s = Search()s.aggs.bucket('articles_per_day', 'date_histogram', field='publish_date', interval='day') \ .metric('clicks_per_day', 'sum', field='clicks') \ .pipeline('moving_click_average', 'moving_avg', buckets_path='clicks_per_day') \ .bucket('tags_per_day', 'terms', field='tags')s.to_dict()# &#123;# "aggs": &#123;# "articles_per_day": &#123;# "date_histogram": &#123; "interval": "day", "field": "publish_date" &#125;,# "aggs": &#123;# "clicks_per_day": &#123; "sum": &#123; "field": "clicks" &#125; &#125;,# "moving_click_average": &#123; "moving_avg": &#123; "buckets_path": "clicks_per_day" &#125; &#125;,# "tags_per_day": &#123; "terms": &#123; "field": "tags" &#125; &#125;# &#125;# &#125;# &#125;# &#125; 4、Sorting12345s = Search().sort( 'category', '-title', &#123;"lines" : &#123;"order" : "asc", "mode" : "avg"&#125;&#125;) 5、Pagination12s = s[10:20]# &#123;"from": 10, "size": 10&#125; 6、Extra Properties and parameters12345678910111213141516171819202122s = Search()# 设置扩展属性使用`.extra()`方法s = s.extra(explain=True)# 设置参数使用`.params()`s = s.params(search_type="count")# 如要要限制返回字段，可以使用`source()`方法# only return the selected fieldss = s.source(['title', 'body'])# don't return any fields, just the metadatas = s.source(False)# explicitly include/exclude fieldss = s.source(include=["title"], exclude=["user.*"])# reset the field selections = s.source(None)# 使用dict序列化一个查询s = Search.from_dict(&#123;"query": &#123;"match": &#123;"title": "python"&#125;&#125;&#125;)# 修改已经存在的查询s.update_from_dict(&#123;"query": &#123;"match": &#123;"title": "python"&#125;&#125;, "size": 42&#125;) 测试代码： https://github.com/zhoujun/mydemos/tree/master/elasticsearch-demo]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>SearchEngine</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Elasticsearch DSL 使用笔记(一)]]></title>
    <url>%2F2017%2F08%2F12%2Felasticsearch-dsl-with-python-usage-1%2F</url>
    <content type="text"><![CDATA[下载地址: https://www.elastic.co/downloads/elasticsearch 使用版本: elasticsearch-5.5.1 Python Elasticsearch DSL: https://github.com/elastic/elasticsearch-dsl-py 测试代码： https://github.com/zhoujun/mydemos/tree/master/elasticsearch-demo 一、Elasticsearch的基本概念 Index：Elasticsearch用来存储数据的逻辑区域，它类似于关系型数据库中的database 概念。一个index可以在一个或者多个shard上面，同时一个shard也可能会有多个replicas。 Document：Elasticsearch里面存储的实体数据，类似于关系数据中一个table里面的一行数据。 document由多个field组成，不同的document里面同名的field一定具有相同的类型。document里面field可以重复出现，也就是一个field会有多个值，即multivalued。 Document type：为了查询需要，一个index可能会有多种document，也就是document type. 它类似于关系型数据库中的 table 概念。但需要注意，不同document里面同名的field一定要是相同类型的。 Mapping：它类似于关系型数据库中的 schema 定义概念。存储field的相关映射信息，不同document type会有不同的mapping。 下图是ElasticSearch和关系型数据库的一些术语比较： Relationnal database Elasticsearch Database Index Table Type Row Document Column Field Schema Mapping Index Everything is indexed SQL Query DSL SELECT * FROM table… GET http://… UPDATE table SET PUT http://… 二、Python Elasticsearch DSL使用简介1、安装1$ pip install elasticsearch-dsl 2、创建索引和文档1234567891011121314151617181920212223242526from datetime import datetimefrom elasticsearch_dsl import DocType, Date, Integer, Keyword, Textfrom elasticsearch_dsl.connections import connections# Define a default Elasticsearch clientconnections.create_connection(hosts=['localhost'])class Article(DocType): title = Text(analyzer='snowball', fields=&#123;'raw': Keyword()&#125;) body = Text(analyzer='snowball') tags = Keyword() published_from = Date() lines = Integer() class Meta: index = 'blog' def save(self, ** kwargs): self.lines = len(self.body.split()) return super(Article, self).save(** kwargs) def is_published(self): return datetime.now() &gt;= self.published_from# create the mappings in elasticsearchArticle.init() 创建了一个索引为blog，文档为article的Elasticsearch数据库和表。必须执行Article.init()方法。 这样Elasticsearch才会根据你的DocType产生对应的Mapping。否则Elasticsearch就会在你第一次创建Index和Type的时候根据你的内容建立对应的Mapping。 现在我们可以通过Elasticsearch Restful API来检查123456789101112131415http GET http://127.0.0.1:9200/blog/_mapping/&#123;&quot;blog&quot;: &#123;&quot;mappings&quot;: &#123;&quot;article&quot;: &#123;&quot;properties&quot;:&#123; &quot;body&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;:&quot;snowball&quot;&#125;, &quot;lines&quot;:&#123;&quot;type&quot;:&quot;integer&quot;&#125;, &quot;published_from&quot;:&#123;&quot;type&quot;:&quot;date&quot;&#125;, &quot;tags&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;, &quot;title&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:&#123;&quot;raw&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;&#125;,&quot;analyzer&quot;:&quot;snowball&quot;&#125; &#125; &#125;&#125; &#125;&#125; 三、使用Elasticsearch进行CRUD操作1、Create an article12345# create and save and articlearticle = Article(meta=&#123;'id': 1&#125;, title='Hello elasticsearch!', tags=['elasticsearch'])article.body = ''' looong text '''article.published_from = datetime.now()article.save() =&gt;Restful API12345678910111213http POST http://127.0.0.1:9200/blog/article/1 title=&quot;hello elasticsearch&quot; tags:=&apos;[&quot;elasticsearch&quot;]&apos;HTTP/1.1 201 CreatedContent-Length: 73Content-Type: application/json; charset=UTF-8&#123; &quot;_id&quot;: &quot;1&quot;, &quot;_index&quot;: &quot;blog&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_version&quot;: 1, &quot;created&quot;: true&#125; 2、Get a article12345678article = Article.get(id=1)# 如果获取一个不存在的文章则返回Nonea = Article.get(id='no-in-es')a is None# 还可以获取多个文章articles = Article.mget([1, 2, 3]) =&gt;Restful API12345678910111213141516171819http GET http://127.0.0.1:9200/blog/article/1HTTP/1.1 200 OKContent-Length: 141Content-Type: application/json; charset=UTF-8&#123; &quot;_id&quot;: &quot;1&quot;, &quot;_index&quot;: &quot;blog&quot;, &quot;_source&quot;: &#123; &quot;tags&quot;: [ &quot;elasticsearch&quot; ], &quot;title&quot;: &quot;hello elasticsearch&quot; &#125;, &quot;_type&quot;: &quot;article&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true&#125; 3、Update a article123456article = Article.get(id=1)article.tags = ['elasticsearch', 'hello']article.save()# 或者article.update(body='Today is good day!', published_by='me') =&gt;Restful API12345678910111213http PUT http://127.0.0.1:9200/blog/article/1 title=&quot;hello elasticsearch&quot; tags:=&apos;[&quot;elasticsearch&quot;, &quot;hello&quot;]&apos;HTTP/1.1 200 OKContent-Length: 74Content-Type: application/json; charset=UTF-8&#123; &quot;_id&quot;: &quot;1&quot;, &quot;_index&quot;: &quot;blog&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_version&quot;: 2, &quot;created&quot;: false&#125; 4、Delete a article12article = Article.get(id=1)article.delete() =&gt; Restful API1234567891011121314151617http DELETE http://127.0.0.1:9200/blog/article/1HTTP/1.1 200 OKContent-Length: 71Content-Type: application/json; charset=UTF-8&#123; &quot;_id&quot;: &quot;1&quot;, &quot;_index&quot;: &quot;blog&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_version&quot;: 4, &quot;found&quot;: true&#125;http HEAD http://127.0.0.1:9200/blog/article/1HTTP/1.1 404 Not FoundContent-Length: 0Content-Type: text/plain; charset=UTF-8]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>SearchEngine</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Python 的开源人脸识别库]]></title>
    <url>%2F2017%2F07%2F30%2Fpython-face-recognition%2F</url>
    <content type="text"><![CDATA[项目说明该库使用 dlib 顶尖的深度学习人脸识别技术构建，在户外脸部检测数据库基准（Labeled Faces in the Wild benchmark）上的准确率高达 99.38%。该项目是要构建一款免费、开源、实时、离线的网络 app，支持组织者使用人脸识别技术或二维码识别所有受邀人员。有了世界上最简单的人脸识别库，使用 Python 或命令行，即可识别和控制人脸。 项目地址：https://github.com/ageitgey/face_recognition#face-recognition 使用说明1、从图片中识别出人脸 123import face_recognitionimage = face_recognition.load_image_file("your_file.jpg")face_locations = face_recognition.face_locations(image) 2、从图片中识别面部特征，比如：眼睛, 鼻子, 嘴和下巴。123import face_recognitionimage = face_recognition.load_image_file("your_file.jpg")face_landmarks_list = face_recognition.face_landmarks(image) 3、可以对识别的面部特征进行处理，比如类似美图秀秀的功能 4、从图片中识别出某个具体的人12345678import face_recognitionknown_image = face_recognition.load_image_file("biden.jpg")unknown_image = face_recognition.load_image_file("unknown.jpg")biden_encoding = face_recognition.face_encodings(known_image)[0]unknown_encoding = face_recognition.face_encodings(unknown_image)[0]results = face_recognition.compare_faces([biden_encoding], unknown_encoding) 更多请查看项目主页： https://github.com/ageitgey/face_recognition#face-recognition]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库分库分表]]></title>
    <url>%2F2017%2F07%2F29%2Fdatabase-sharding%2F</url>
    <content type="text"><![CDATA[一、背景 大量的并发读/写操作，导致单库出现难以承受的负载压力 单表存储数据量过大，导致索引效率低下 二、读写分离将数据库设置为读写分离状态，由Master（主）负责写数据，Salve（从）只负责读数据。主从之间保持数据同步。根据二八法则，80%的数据库操作是读，20%则为写。读写分离后，可以大大提升单库负载压力。 但是，当Master存在TPS（系统吞吐量）较高的情况，Master和Slave数据库之间数据同步是会存在一定延迟，因此在写入Master之前最好还是要将同一份数据放入缓存中，以避免高并发情况下，从Slave中获取不到指定数据的情况 三、分库分表随着用户规模的不断上升，仅仅只是依靠数据库的读写分离并不能解决系统瓶颈。因此需要考虑分库分表的解决方案。 分表 就是将原来冗余在单库中的单个业务表拆分为N个“逻辑相关”的业务子表（如tab_0000, tab_0001, tab_0002…），不同的业务字表各自负责存储不同区间数据，对外形成一个整体 分库 就是将分表后的业务子表按照特定的算法和规则分散到N个“逻辑相关”的业务子库中（如db_0000, db_0001, db_0002…） 分库分表策略主要原理：分区，取模，数据路由表 1、按时间区间一定时间区间内产生的数据放到一张表里面，多个时间区间的表放到一个库里面 比如， a、单库多表结构，按月分表可以这样：user_201701, user_201702…user_201712；按年分表可以这样，user_2016, user_2017… b、多库多表，比如按天分表，每天一张表，当单库超过100张表的时候，进行分库到下一张表。那么假如第一张表在库db0，表名是user_20160201。从db0.user_20160201 - db0.user_20160511就是100张表了，接下来就是分库，进入20160512，就是db1.user_20160512，这个算法就是上线的时候定一个上线日期，具体算法如下： 12库ID = （当前日期 - 上线日期）/ 100表ID = user_yyyyMMdd 注：好处是可以直接根据时间经过简单计算定位到哪个库和哪个表 还有一种算法：12表ID = (当前日期 - 上线日期) % 100表名如下： DB0.user_0001，user_0002 .... user_01000 注：表名和库名都要经过计算，比较麻烦 c、按月分表，每个月一张表。这种情况，一般就不用分库了，一年12张表说明量也不会特别大，如果量特别大，或者是热点数据，可以一年分一个库，具体算法和上面差不多。 d、按季度分表，按年度分表（基本不用分库） 2、按主键ID区间对于自增的主键ID，可以按照ID区间进行分表，以1000万数据量为分界线，对线性的ID进行切割分表，每涨到1000万数据，分到下一张表，超过一定数目的表，进行分库。 12库ID = 主键ID / 1000万 / 100表ID = 主键ID / 1000万 % 100 如： 12db0.user_0000 ... db0.user_0099 db1.user_0000 ... db1.user_0099 3、按照用户ID取模这里把按照用户ID取模单独拎出来，因为就使用而言，是使用场景最多的情况，很多时候都是用户相关数据量最大，需要分库分表，查询维度更多也是按照用户来查询，所以对用户取模，让同一个用户的数据落到一张表里面，再好不过了。 这里模式用户ID是整数型的。假设库数量要分4库，每个库表数量8表，一共32张表。 12345库ID = userId % 库数量4表ID = userId / 库数量4 % 表数量8或者库ID = userId / 表数量4 % 库数量4表ID = userId % 表数量8 4、按照指定字段hash后再取模如果要取模的字段不是整数型，要先hash后，再通过取模算法，算出在哪个库和那个表。具体算法，参照上面的按用户ID取模。 5、数据路由表如果分库分表的算法很复杂，可以通过路由表+程序算法，来存储和计算分库分表规则，不过一般不建议，分库分表搞得太复杂，不便于维护和查询问题 四、第三方中间件1、Cobar 开源地址：https://github.com/alibaba/cobar 2、Shark 开源地址：https://github.com/gaoxianglong/shark 五、需要考虑的问题一旦数据库实施分库分表后，便会对开发造成一定的影响。具体问题如下： 数据的原子性，一致性，隔离性，持久性如何保证 多表之间的关联查询如何进行 无法继续使用外键约束 无法继续使用Oracle提供的Sequence或MySQL提供的AUTO_INCREMENT生成全局唯一和连续性ID。 参考文献 https://baijiahao.baidu.com/s?id=1564548884823698&amp;wfr=spider&amp;for=pc http://blog.csdn.net/xlgen157387/article/details/53976153 http://blog.csdn.net/dinglang_2009/article/details/53195871]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>Shard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy 部署笔记]]></title>
    <url>%2F2017%2F07%2F02%2Fscrapy-deploy-usage%2F</url>
    <content type="text"><![CDATA[关键字： scrapy， scrapyd， scrapy-client， spiderkeeper 一、新建一个scrapy项目12345678910$ scrapy startproject scrapy_example$ cd scrapy_example# 修改配置文件scrapy.cfg[settings]default = scrapy_example.settings[deploy:scrapyd1]url = http://localhost:6800/project = scrapy_example 二、安装并启动scrapyd1234$ pip install scrapyd# 启动, 默认的端口是6800。可以在浏览器中查看结果，比如：http://127.0.0.1:6800/。$ scrapyd 三、发布工程到scrapyd12345678910# 发布scrapyd需要安装scrapy-client$ pip install scrapy-client# 发布命令：scrapyd-deploy &lt;target&gt; -p &lt;project&gt;$ scrapyd-deploy scrapyd1 -p scrapy_example# 检查发布是否成功$ scrapyd-deploy -l 或者 scrapyd-deploy -L scrapyd1# 启动爬虫$ curl http://127.0.0.1:6800/schedule.json -d project=scrapy_example -d spider=example 四、使用spiderkeeper发布爬虫12345678910# 生成egg文件$ scrapyd-deploy --build-egg scrapy_example.egg# 安装spiderkeeper(https://github.com/DormyMo/SpiderKeeper)# pip install spiderkeeper# 启动spiderkeeper$ spiderkeeper --server=http://localhost:6800# 访问 http://localhost:5000# 这个时候可以在spiderkepper上传scrapy_example.egg文件，通过Web UI启动，监控爬虫 参考文献http://www.cnblogs.com/jinhaolin/p/5033733.html]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
        <tag>Scrapyd</tag>
        <tag>SpiderKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 构建第一个App]]></title>
    <url>%2F2017%2F06%2F22%2Fdocker-build-and-run-your-first-app%2F</url>
    <content type="text"><![CDATA[测试环境：阿里云 ubuntu14.04 一、新建Dockerfile文件1234567891011121314151617181920# Use an official Python runtime as a base imageFROM python:2.7-slim# Set the working directory to /appWORKDIR /app# Copy the current directory contents into the container at /appADD . /app# Install any needed packages specified in requirements.txtRUN pip install -r requirements.txt# Make port 80 available to the world outside this containerEXPOSE 80# Define environment variableENV NAME World# Run app.py when the container launchesCMD [&quot;python&quot;, &quot;app.py&quot;] 二、新建requirements.txt12FlaskRedis 三、新建app.pyfrom flask import Flask from redis import Redis, RedisError import os import socket # Connect to Redis redis = Redis(host=&quot;redis&quot;, db=0, socket_connect_timeout=2, socket_timeout=2) app = Flask(__name__) @app.route(&quot;/&quot;) def hello(): try: visits = redis.incr(&quot;counter&quot;) except RedisError: visits = &quot;&lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;&quot; html = &quot;&lt;h3&gt;Hello {name}!&lt;/h3&gt;&quot; \ &quot;&lt;b&gt;Hostname:&lt;/b&gt; {hostname}&lt;br/&gt;&quot; \ &quot;&lt;b&gt;Visits:&lt;/b&gt; {visits}&quot; return html.format(name=os.getenv(&quot;NAME&quot;, &quot;world&quot;), hostname=socket.gethostname(), visits=visits) if __name__ == &quot;__main__&quot;: app.run(host=&apos;0.0.0.0&apos;, port=80) 四、编译 ls docker build -t friendlyhello . (当前目录) docker images 五、运行 docker run -p 4000:80 friendlyhello 六、测试 curl http://localhost:4000 参考文献https://docs.docker.com/get-started/part2/#run-the-app]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Docker</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Supervisord 使用笔记]]></title>
    <url>%2F2017%2F06%2F22%2Fsupervisord-usage%2F</url>
    <content type="text"><![CDATA[一、安装1234- pip install supervisor - echo_supervisord_conf #或者 - echo_supervisord_conf &gt; supervisord.conf - supervisord -c ./supervisord.conf 二、遇到错误123[root@localhost ~]# supervisord -c ./supervisord.conf Error: .ini file does not include supervisord section For help, use /usr/bin/supervisord -h 错误原因是因为配置文件少了 [supervisrod] 配置项12345678910[program:djangotest] user=lzz command=/usr/bin/python /var/www/djangotest/manage.py runserver 0.0.0.0:8000 autostart=true autorestart=true stderr_logfile=/var/logs/err.log stdout_logfile=/var/logs/out.log stopsignal=INT [supervisord] 三、常用命令12supervisord : supervisor的服务器端部分，用于supervisor启动supervisorctl：启动supervisor的命令行窗口，在该命令行中可执行start、stop、status、reload等操作。 参考文献http://www.jianshu.com/p/9abffc905645]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Supervisord</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 安装svn服务]]></title>
    <url>%2F2017%2F01%2F27%2Fubuntu-install-svn-simple-server%2F</url>
    <content type="text"><![CDATA[一、安装Subversion Server1apt-get install subversion 二、创建SVN版本库12mkdir /srv/svn # 创建SVN根目录svnadmin create test # 创建测试项目库 三、SVN配置主要包含以下三个配置文件： 1. svnserve.conf服务配置：123456[general]anon-access = none # 匿名用户不可读auth-access = write # 权限用户可写password-db = passwd # 启用密码文件authz-db = authz # 启用权限文件realm = repos # 认证域名称 2. passwd账号配置：12[user]finger = 123456 3. authz权限配置：123456[groups]admin = finger[/]@admin = rw # admin组拥有所有读写权限* = r # 其他只有读权限 四、启动和停止1234567启动：svnserve -d -r /srv/svn # -d 表示后台运行 -r 表示根目录停止：ps -ef | grep svn # 查看svn进程IDkill -9 进程ID 五、测试1svn co svn://127.0.0.1/test 六、问题1、svn: E220003: Invalid authz configuration 原因是authz文件配置错误，仔细检查authz文件。 七、参考文献https://my.oschina.net/jast90/blog/382688 http://www.linuxidc.com/Linux/2015-01/111956.htm]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cocos2dx－上传Google Play遇到的问题]]></title>
    <url>%2F2017%2F01%2F27%2Fcocos2dx-upload-google-play%2F</url>
    <content type="text"><![CDATA[一、OpenSSL版本有多个安全漏洞，建议您尽快更新 OpenSSL 因为项目使用的是cocos2d-x 3.3版本。因此cocos2d-x第三方库比较旧。上传Google Play 后台时被rejected。邮件原文如下： 12345678910111213141516171819202122Vulnerability-----------------------------------------------------------**OpenSSL**The vulnerabilities were addressed in OpenSSL 1.0.2f/1.0.1r. To confirm your OpenSSL version, you can do a grep search for:\$ unzip -p YourApp.apk | strings | grep &quot;OpenSSL&quot;You can find more information and next steps in this Google Help Center article.-----------------------------------------------------------To confirm you’ve upgraded correctly, submit the updated version of your app to the Play Console and check back after five hours to make sure the warning is gone.While these vulnerabilities may not affect every app that uses this software, it’s best to stay up to date on all security patches. Make sure to update any libraries in your app that have known security issues, even if you&apos;re not sure the issues are relevant to your app.Apps must also comply with the Developer Distribution Agreement and Developer Program Policies.If you feel we have made this determination in error, please reach out to our developer support team.Best,The Google Play Team 解决方案 邮件写的很清楚，主要是OpenSSL的库版本太低，存在漏洞。cocos论坛上也有很多同学遇到类似的问题，传送门：http://discuss.cocos2d-x.org/t/the-vulnerabilities-were-addressed-in-openssl-1-0-2f-1-0-1r/35766/9 具体做法，就是从cocos的第三方库下载最新的curl库替换当前项目的curl。 https://github.com/cocos2d/cocos2d-x-3rd-party-libs-bin 二、Google Play支付(in-app Billing)注意：在测试之前一定要注意后台APK版本号、签名，一定要与测试用的APK版本号签名一致 1、错误提示 1231. &quot;需要验证身份，您需要登录自己google账号&quot; // 没有成功发布apk测试(Alpha)版本2. &quot;无法购买您要买的商品&quot; // 已成功发布Alpah版本。需要等待生效 2、系统异常 123456789101112131415java.lang.IllegalStateException: Can&apos;t start async operation (launchPurchaseFlow)// 解决方法// 1、mHelper.flagEndAsync() 修改成public// 2、在launchPurchaseFlow() 之前调用flagEndAsync()----------------------------------------------------if (mHelper != null) &#123; try &#123; mHelper.flagEndAsync(); mHelper.launchPurchaseFlow(this, item, RC_REQUEST, mPurchaseFinishedListener, &quot;&quot;); &#125; catch(IllegalStateException ex)&#123; Toast.makeText(this, &quot;Please retry in a few seconds.&quot;, Toast.LENGTH_SHORT).show(); &#125;&#125;]]></content>
      <categories>
        <category>Cocos2d-x</category>
      </categories>
      <tags>
        <tag>Cocos2d-x</tag>
        <tag>Javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo使用笔记]]></title>
    <url>%2F2017%2F01%2F27%2Fhexo-usage%2F</url>
    <content type="text"><![CDATA[中文文档：https://hexo.io/zh-cn/docs/index.html 参考博客：http://blog.csdn.net/poem_of_sunshine/article/details/29369785/http://www.jianshu.com/p/465830080ea9#http://www.jianshu.com/p/a2023a601ceb Github配置：https://help.github.com/articles/generating-an-ssh-key/https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/ 常用命令1234hexo server | s --debughexo cleanhexo generatehexo deploy 绑定阿里云域名：http://quantumman.me/blog/setting-up-a-domain-with-gitHub-pages.html 后端管理插件hexo-admin123npm install --save hexo-adminhexo server -dopen http://localhost:4000/admin/ 设置后台密码修改站点配置文件，就是网站根目录下的 _config.yml文件:1234admin: username: finger password_hash: $2a$1$Cof3VuvY8nKKIUjeCNBSE.HjcrCKQ1P80GEegP//SLDFWZoGzm4pa secret: a secret something username：后端登录用户名 password_hash：后端登录用户密码对应的md5 hash值 secret：用于保证cookie安全 密码生成hexo-admin密码是bcrypt编码。因此需要安装bcrypt-nodejs模块1234$ node&gt; const bcrypt = require(&apos;bcrypt-nodejs&apos;)&gt; bcrypt.hashSync(&apos;your password secret here!&apos;)//=&gt; &apos;$2a$10$8f0CO288aEgpb0BQk0mAEOIDwPS.s6nl703xL6PLTVzM.758x8xsi&apos; 在线生成https://www.bcrypt-generator.com/ Hexo主题： https://hexo.io/themes/ https://github.com/henryhuang/oishi https://github.com/haojen/hexo-theme-Anisina https://github.com/stiekel/hexo-theme-random https://github.com/SuperKieran/TKL https://github.com/iissnan/hexo-theme-next Next 皮肤设置http://theme-next.iissnan.com/theme-settings.html]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github 本地配置]]></title>
    <url>%2F2017%2F01%2F13%2Fgithub-config-usage%2F</url>
    <content type="text"><![CDATA[本地配置github用户名密码1、使用命令123git config --global user.name [username]git config --global user.email [email]git config --global credential.helper store 2、配置文件123[user] name = finger email = finger_chou@163.com 保存用户名密码1、使用命令12echo &quot;[credential]&quot; &gt;&gt; .git/configecho &quot; helper = store&quot; &gt;&gt; .git/config 2、配置文件12[credential] helper = store 查看配置1git config --list]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python不同目录下的调用情况]]></title>
    <url>%2F2017%2F01%2F02%2Fpython-different-directories-execute%2F</url>
    <content type="text"><![CDATA[Python包含子目录中的模块方法比较简单，关键是能够在sys.path里面找到通向模块文件的路径。 下面将具体介绍几种常用情况: （1）主程序与模块程序在同一目录下:如下面程序结构: 123`-- src |-- mod1.py `-- test1.py 若在程序test1.py中导入模块mod1, 则直接使用import mod1或from mod1 import *; （2）主程序所在目录是模块所在目录的父(或祖辈)目录如下面程序结构: 12345`-- src |-- mod1.py |-- mod2 | `-- mod2.py `-- test1.py 若在程序test1.py中导入模块mod2, 需要在mod2文件夹中建立空文件init.py文件(也可以在该文件中自定义输出模块接口); 然后使用 from mod2.mod2 import * 或import mod2.mod2. （3）主程序导入上层目录中模块或其他目录(平级)下的模块如下面程序结构: 1234567`-- src |-- mod1.py |-- mod2 | `-- mod2.py |-- sub | `-- test2.py `-- test1.py 若在程序test2.py中导入模块mod1和mod2。首先需要在mod2下建立init.py文件(同(2))，src下不必建立该文件。然后调用方式如下:下面程序执行方式均在程序文件所在目录下执行，如test2.py是在cd sub;之后执行python test2.py而test1.py是在cd src;之后执行python test1.py; 不保证在src目录下执行python sub/test2.py成功。 1234import syssys.path.append(&quot;..&quot;)import mod1import mod2.mod2 （4）从(3)可以看出，导入模块关键是能够根据sys.path环境变量的值，找到具体模块的路径。这里仅介绍上面三种简单情况]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>